<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
   xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
   xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#"
   xmlns:rdfe="http://redfoot.net/rdf#"
   xmlns:redfoot="http://redfoot.net/2005/redfoot#"
>

  <rdfe:RDFXMLDocument rdf:about="">
    <rdfs:label>Redfoot Kernel RDF/XML</rdfs:label>
  </rdfe:RDFXMLDocument>
  
  <redfoot:Kernel rdf:about="#">
    <rdfs:label>Redfoot Kernel</rdfs:label>
    <rdfs:comment>
    </rdfs:comment> 
    <redfoot:program rdf:resource="programs/command_runner#"/>
    <rdf:value rdf:datatype="http://redfoot.net/2005/redfoot#Python">
<![CDATA[

"""
Redfoot an application for managing and running hypercode. And includes hypercode for building websites.

For help on Redfoot see:

   redfoot.py help

"""

import sys, getopt
import traceback
import warnings
from types import ModuleType
from urlparse import urljoin, urldefrag, urlparse
from urllib import pathname2url, url2pathname
from itertools import chain
import os
import logging

_logger = logging.getLogger("%s" % redfoot_loader.label(redfoot_program, redfoot_program))

import rdflib

version = tuple([int(x) for x in rdflib.__version__.split(".", 2)])    
MINIMUM = (2, 3, 3)
if version < MINIMUM:
    _logger.critical("This kernel requires rdflib %s or greater required. Found rdflib version %s. Run redfoot --install-rdflib or see http://rdflib.net/ for information on downloading and installing rdflib." % ("%s.%s.%s" % MINIMUM, rdflib.__version__))
    sys.exit(-1)

from rdflib import RDF, RDFS, Namespace, StringInputSource
from rdflib import URIRef, BNode, Literal
from rdflib.Graph import Graph, ConjunctiveGraph
from rdflib.util import first, date_time
from rdflib.Journal import JournalReader, JournalWriter

redfoot_base = redfoot_loader.program.split("hypercode/kernel#")[0]

def logical_to_physical(uri):
    return URIRef(uri.replace("http://redfoot.net/", redfoot_base, 1))

def physical_to_logical(uri):
    return URIRef(uri.replace(redfoot_base, "http://redfoot.net/", 1))

redfoot_program = physical_to_logical(redfoot_program)

import urllib2, re
from datetime import datetime, timedelta

NAMESPACE = URIRef("http://redfoot.net/2005/redfoot#")

DC_creator = URIRef("http://purl.org/dc/elements/1.1/creator")
DC_created = URIRef("http://purl.org/dc/terms/created")

HTTP = Namespace("http://www.w3.org/1999/xx/http#")
OWL = Namespace("http://www.w3.org/2002/07/owl#")
SCUTTER = Namespace("http://redfoot.net/2006/scutter#")

LINK_PREDICATES = [OWL.imports,RDFS.isDefinedBy,RDFS.seeAlso]

HTTP_CACHE_PREDICATES = [
    HTTP['Date'],
    HTTP['Last-Modified'],
    HTTP['Content-Type'],
    HTTP['ETag'],
]

HEADER_KEYS = [
    'Date',
    'Last-Modified',
    'Content-Type',
    'ETag',
]

SCUTTER_STEPS = 5

HTTP_CACHE_TTL = 24 * 60 * 60 #One day

class HTTPGetEvent:
    """
    Represents a prior HTTP GET by a scutter (a dated event).  Provides functionality for
    managing HTTP headers for subsequent fetches (to the same location)
    """
    def __init__(self,provenanceGraph,location,publicID=None,eventId=None):
        if eventId:
            assert isinstance(eventId,BNode),"Scutter event identifiers should be BNodes, not %s"%(type(eventId))
            self.identifier = eventId
        else:
            event = first(provenanceGraph.subjects(SCUTTER.fetch,URIRef(location)))
            
            if event:
                self.identifier = event
            else:
                self.identifier = BNode()
                provenanceGraph.add((self.identifier,SCUTTER.fetch,URIRef(location)))
                provenanceGraph.add((self.identifier, DC_created, Literal(date_time())))
                provenanceGraph.add((self.identifier, RDF.type, SCUTTER.Event))
                if publicID:
                    provenanceGraph.add((self.identifier, SCUTTER.target, URIRef(publicID)))
                
        self.provenanceGraph = provenanceGraph

    def __get_expiration_date(self):
        now=datetime.now().isoformat().split('.')[0]
        expiration = first(self.provenanceGraph.objects(self.identifier,SCUTTER.httpCacheExpiration))
        #_logger.info(" ".join([now,expiration,str(expiration > now)]))
        return first(self.provenanceGraph.objects(self.identifier,SCUTTER.httpCacheExpiration))
    expirationDate = property( __get_expiration_date)

    def __get_public_id(self):
        return self.provenanceGraph.value(subject=self.identifier,predicate=SCUTTER.target, any=True)
    publicID = property( __get_public_id)

    def __get_system_id(self):
        return self.provenanceGraph.value(subject=self.identifier,predicate=SCUTTER.fetch, any=True)
    systemID = property(__get_system_id)        

    def cacheEvent(self,httpStream,parseFormat):
        """
        Cache HTTP header information from the given HTTP stream
        parseFormat is the format (xml or n3) that was used to successfully parse.  Setup the cache expiration
        the RDF graph and is used as a fallback if the content-type doesn't match (text/plain, for instance)
        """
        #_logger.info("Caching HTTP headers for URL: %s"%(first(self.provenanceGraph.objects(self.identifier,SCUTTER.fetch))))
        now = datetime.now()
        expirationDT = (now + timedelta(seconds = HTTP_CACHE_TTL)).isoformat().split('.')[0]
        #_logger.info(" ".join([now.isoformat(),str(HTTP_CACHE_TTL),expirationDT]))
        self.provenanceGraph.remove((self.identifier,SCUTTER.httpCacheExpiration,None))
        self.provenanceGraph.add((self.identifier, SCUTTER.httpCacheExpiration, Literal(expirationDT)))
        for header_key in HEADER_KEYS:
            val = httpStream.info().get(header_key)
            if header_key == 'Content-Type':
                isXml = re.match(r'(?:text|application)/.*\+?xml',val) is not None
                if not isXml and parseFormat=='n3':
                    val = 'text/n3'
                elif not isXml and parseFormat=='xml':
                    val = 'application/rdf+xml'
                
            if val:
                #_logger.info("Caching HTTP header (%s): %s"%(header_key,val))
                self.provenanceGraph.add((self.identifier,HTTP[header_key],Literal(val)))

    def invalidateCache(self):
        """
        Clear the HTTP header metadata associated with this event. This would be called if
        the content at the location was updated
        """
        now = datetime.now()
        expirationDT = (now + timedelta(seconds = HTTP_CACHE_TTL)).isoformat().split('.')[0]
        self.provenanceGraph.remove((self.identifier,SCUTTER.httpCacheExpiration,None))
        self.provenanceGraph.add((self.identifier, SCUTTER.httpCacheExpiration, Literal(expirationDT)))
        for http_cache_pred in HTTP_CACHE_PREDICATES:
            self.provenanceGraph.remove((self.identifier,http_cache_pred,None))

    def bumpExpiration(self):
        """
        Extend the cache expiration.  This is called because the cache expired and
        a (cacheable) subsequent request responded in a 304 (no change at the server).
        This should probably bump the cache by an increasing amount so static RDF graphs
        (like the owl.rdfs) aren't repeatably hit uneccessarily
        """
        now = datetime.now()
        expirationDT = (now + timedelta(seconds = HTTP_CACHE_TTL)).isoformat().split('.')[0]
        self.provenanceGraph.remove((self.identifier,SCUTTER.httpCacheExpiration,None))
        self.provenanceGraph.add((self.identifier, SCUTTER.httpCacheExpiration, Literal(expirationDT)))

    def createHTTPHeaders(self):
        """
        Return a dictionary of HTTP headers to be used for a subsequent fetch to
        the location associated with this event.  This is mechanism by which
        HTTP caching and content negotiation is faciliated
        """
        headers={}        
        httpDate = first(self.provenanceGraph.objects(self.identifier,HTTP['Date']))
        httpLastModified = first(self.provenanceGraph.objects(self.identifier,HTTP['Last-Modified']))
        httpContentType = first(self.provenanceGraph.objects(self.identifier,HTTP['Content-Type']))
        httpETag = first(self.provenanceGraph.objects(self.identifier,HTTP['ETag']))

        if httpDate:
            headers['If-Modified-Since']= httpDate
            
        if httpETag:
            headers['If-None-Match']    = httpETag

        headers['Accept']               = httpContentType
        headers['User-agent']           = 'Redfoot 2.0.X'

        isXml = re.match(r'(?:text|application)/.*\+?xml',httpContentType) is not None

        return headers, isXml and 'xml' or 'n3'

from urllib2 import urlopen, Request

from xml.sax.xmlreader import InputSource

from rdflib import __version__

class UnknownURLInputSource(InputSource, object):
    def __init__(self, location):
        super(UnknownURLInputSource, self).__init__(location)
        self.valid = False
        try:
            httpStream = urllib2.urlopen(urllib2.Request(location))
            self.valid = True
            self.contentType = httpStream.info().get('content-type')
            self.format = None
            if re.match(r'.*html', self.contentType) is not None:
                self.format = "html"
            elif re.match(r'(?:text|application)/.*\+?xml', self.contentType) is not None:
                self.format = "xml"
            elif re.match(r'.*n3', self.contentType) is not None:
                self.format = 'n3'
            self.setByteStream(httpStream)
        except Exception, e:
            _logger.warning("Unable to connect to %s: %s\n"%(linkURL,e))
        
class CacheableURLInputSource(InputSource, object):
    def __init__(self, priorEvent):
        super(CacheableURLInputSource, self).__init__(priorEvent.publicID)
        self.modified = True
        if priorEvent:
            hdrs, format = priorEvent.createHTTPHeaders()
            try:
                httpStream = urllib2.urlopen(urllib2.Request(system_id, headers=hdrs))
            except urllib2.HTTPError, e:
                _logger.info("Recieved 304 status from server.  Cached content is sufficient")
                self.modified = False
                assert e.code == 304, "HTTP response code %s recieved after cacheable request to %s"%(e.code,self.url)
                #HTTP Error 304: Not Modified, do nothing
                httpStream = None
                # TODO: we'll likely need to deal with other http errors

        if httpStream:
            contentType = httpStream.info().get('content-type')
            format = None
            if re.match(r'.*html', contentType) is not None:
                format = "html"
            elif re.match(r'(?:text|application)/.*\+?xml', contentType) is not None:
                format = "xml"
            elif re.match(r'.*n3', contentType) is not None:
                format = 'n3'
            self.format = format
            self.setByteStream(httpStream)

    def __repr__(self):
        return self.url


class Redfoot(ConjunctiveGraph):

    def __init__(self, store):
        super(Redfoot, self).__init__(store)
        self.scutterFailureCache = {}
	self.__base = None # default to CWD
        self.__log = None        
        self.__index = None
        self.__config = None
        self.__xmpp = None
        self.__xmpp_id = None
        self.__xmpp_password = None
        self.__xmpp_host = None
        self.__xmpp_port = None 
        self.__rule_engine = None
        
    def absolutize(self, uri, defrag=1):
        base = self.base
        result = urljoin(base, uri, allow_fragments=not defrag)
        if not defrag:
            if uri and uri[-1]=="#" and result[-1]!="#":
                result = "%s#" % result
        else:
            result, frag = urldefrag(result)
        return URIRef(result)

    def __get_base(self):
        if self.__base is None:
            self.__base = self.value(REDFOOT.Globals, REDFOOT.base)
            if self.__base is None:
                self.__base = URIRef("%s/" % urljoin("file:", pathname2url(os.getcwd())))
        return self.__base
    def __set_base(self, base):
	self.config.remove((REDFOOT.Globals, REDFOOT.base, None))
	self.config.add((REDFOOT.Globals, REDFOOT.base, base))
    base = property(__get_base, __set_base)

    def _get_log(self):
        if self.__log is None:
            warnings.warn("Use a local logger. Ex. _logger = logging.getLogger('my_module')",
                          DeprecationWarning, stacklevel=2)
            self.__log = _logger
        return self.__log
    log = property(_get_log)

    def __get_program(self):
        # redfoot_program is defined by the BootLoader and is the boot program
        program = self.value(redfoot_program, REDFOOT.program)
        program = program or redfoot_loader.value(redfoot_program, REDFOOT.program)
        assert program, "No default program found"            
        return program
    # program resource redfoot will execute
    program = property(__get_program)
    
    def __get_config(self):
        config = self.__config
        if config is None:
           config =  self.__config = self.get_context(BNode("_config"), creator=redfoot_loader.program)
        return config
    # context where redfoot stores configuration data
    config = property(__get_config)

    def __get_index(self):
        if self.__index is None:
            id = BNode("_index")
            self.__index = Graph(store=self.store, identifier=id, namespace_manager=self)
            self.get_context(id, creator=redfoot_loader.program) # to add creator and other bits that get_context adds
        return self.__index
    # context where redfoot stores data about contexts
    index = property(__get_index)
            
    def open(self, path):
        #super(Redfoot, self).open(path) # TODO: why does this not work?
        self.store.open(path)

    def _bootstrap(self):
        # Note: we are currently being passed an open store        
        global REDFOOT
        # bootstraping REDFOOT namespace        
        #if not (NAMESPACE, None, None) in self:
        #    context = Graph()
        #    context.load(logical_to_physical(NAMESPACE), publicID=NAMESPACE)
        #else:
        #    context = Graph(store=self.store, identifier=self.context_id(NAMESPACE), namespace_manager=self)
        #REDFOOT = self.namespace(NAMESPACE, context=context)
        REDFOOT = Namespace(NAMESPACE)
        self.check(NAMESPACE, creator=physical_to_logical(redfoot_loader.program))
        REDFOOT = self.namespace(NAMESPACE, creator=physical_to_logical(redfoot_loader.program))
        
        self.uri = redfoot_program # uri to the version of redfoot that's running
        context = dict({"redfoot": self, "REDFOOT": REDFOOT, 
                        "RDF": RDF, "RDFS": RDFS,
                        "URIRef": URIRef, "BNode": BNode, "Literal": Literal
                        })
        self.__context = context

    def label(self, subject, default=''):
        # TODO: push this subproperty support back down into rdflib
        label = super(Redfoot, self).label(subject, None)
        if label is None:
            for subproperty in self.transitive_subjects(RDFS.subPropertyOf, RDFS.label):
                label = self.value(subject, subproperty, default=None, any=True)        
                if label is not None:
                    return label
        return label or default

    def check(self, uri, creator=None):
        """
        Checks to see if redfoot knows anything about uri

        Currently, if not, Redfoot will attempt to load from uri.
        """
        if isinstance(uri, URIRef):
            location = uri.defrag()
            context_uri = self.context_id(location)
            if not (context_uri, RDF.type, REDFOOT.Context) in self.index: 
                c = self.get_context(context_uri) #self.index.add((context_uri, RDF.type, REDFOOT.Context))
                if uri==location:
                    _logger.info("loading: %s" % uri)
                else:
                    _logger.info("loading: %s from %s" % (uri, logical_to_physical(location)))
                try:
                    context = self.load(location, creator=creator)
                except Exception, e:
                    _logger.warning("couldn't load %s while checking: %s\n" % (uri, e))

    def scutter(self, location, creator, steps=SCUTTER_STEPS):
        """
        Performs a recursive scutter from the given location.  This assumes that the location has already
        been loaded / cached prior to this function being called.  It attempts to load references (via link predicates)
        as URLs, recursively (breadth-first) calling scutter on each URL that was successfully loaded as an RDF graph .  It does
        this no more than SCUTTER_STEPS times.  Content negotiation is used to perform discovery of uknown URLs in order
        to determine how to parse remote content.
        """
        _logger.info("Scuttering from %s"%location)
        visitedNodes = []
        sourceGraph = self.get_context(self.context_id(location),creator=creator)
        for linkPredicate in LINK_PREDICATES:
            linkedURLs = []
            try:
                linkedURLs = sourceGraph.objects(predicate=linkPredicate)
            except Exception,e:
                _logger.info(e)

            for linkURL in linkedURLs:
                #If the link is to itself or it has been attempted before, ignore
                if "#" in linkURL:
                    uri, frag = urldefrag(linkURL)
                    linkURL = URIRef(uri)

                if linkURL == location or linkURL in self.scutterFailureCache:
                    continue
                #Skip the rest if the maximum number of recursion steps have been surpassed
                if steps < 1:
                    break

                #Check if linkURL has already been loaded
                priorEvent = self.index.value(predicate=SCUTTER.fetch, object=linkURL, any=True)
                if priorEvent:
                    priorEvent = HTTPGetEvent(self.index, linkURL, priorEvent)
                    if priorEvent.expirationDate < datetime.now().isoformat().split('.')[0]:
                        _logger.info("%s has already been loaded previously but the cache has expired"%linkURL)
                        #Aleady loaded, attempt a cacheable, content-negotiated load with HTTP provenance data on RDF graph URL
                        cacheableSource = CacheableURLInputSource(priorEvent)
                        if cacheableSource.modified:
                            self.load(linkURL, publicID=priorEvent.publicID,format=cacheableSource.format,scutter=False)
                        else:
                            priorEvent.bumpExpiration()
                            
                    visitedNodes.append(linkURL)
                    steps -= 1

                else:
                    #_logger.info("URL %s (linked by %s) has not been loaded before.. Attempting RESTful discovery"%(linkURL,linkPredicate))
                    #Hasn't been loaded.  Need to perform 'RESTful' scutter discovery
                    #Try to connect, first
                    unknownURL = UnknownURLInputSource(linkURL)
                    if not unknownURL.valid:
                        continue

                    #If the format is known (HTML is ignored), parse it using the format recorded at
                    #the server, otherwise ignore the URL (Perhaps this is too strict?)
                    if unknownURL.format:
                        if unknownURL.format=="html":
                            #_logger.warning("Ignoring html formatted url: %s" % (linkURL))
                            self.scutterFailureCache[linkURL]=None
                        else:
                            try:
                                self.load(linkURL, format=unknownURL.format,scutter=False)
                                visitedNodes.append(linkURL)
                                steps -= 1
                            except:
                                _logger.warning("Couldn't parse %s using the given format: %s" % (linkURL,unknownURL.format))
                                self.scutterFailureCache[linkURL]=None
                    else:
                        #Need to attempt an Notation 3 parse first, then an RDF/XML parse
                        try:
                            self.load(linkURL, format='n3',scutter=False)
                            visitedNodes.append(linkURL)
                            steps -= 1
                        except:
                            try:
                                self.load(linkURL,scutter=False)
                                visitedNodes.append(linkURL)
                                steps -= 1                                
                            except Exception,e:
                                _logger.warning("Unable to parse %s as either N3 or RDF/XML: %s" % (linkURL, e))
                                self.scutterFailureCache[linkURL]=None
                                
        visitedNodes = dict([(location,None) for location in visitedNodes]).keys()                   
        for visitedLocation in visitedNodes:
            self.scutter(visitedLocation,creator,steps=steps)

    def load(self, location, format="xml", publicID=None, creator=None, scutter=False):
        """
        location is a relative or absolute URI
        publicID if specified will override location as the xml:base
        contextID
        """
        if "#" in location:
            location = URIRef(urldefrag(location)[0])
        location = logical_to_physical(location)
        publicID = publicID or self.absolutize(location)
        publicID = self.absolutize(publicID, defrag=False)
        publicID = physical_to_logical(publicID)
        context_id = self.context_id(publicID)
        context = self.get_context(context_id, creator=creator)

        absolute_location = self.absolutize(location)
        context.remove((None, None, None))
        if scutter is True and urlparse(absolute_location)[0] == 'http':
            #priorEvent = self.index.value(predicate=SCUTTER.target, object=publicID, any=True)
            priorEvent = self.index.value(predicate=SCUTTER.fetch, object=absolute_location, any=True)
            if priorEvent:
                priorEvent = HTTPGetEvent(self.index, absolute_location, priorEvent)
                httpStream = urllib2.urlopen(urllib2.Request(absolute_location, headers=priorEvent.createHTTPHeaders()[0]))
                priorEvent.invalidateCache()
                priorEvent.cacheEvent(httpStream, format)
            else:
                newEvent = HTTPGetEvent(self.index, absolute_location)
                httpStream = urllib2.urlopen(urllib2.Request(absolute_location))
                newEvent.cacheEvent(httpStream, format)
            context.load(httpStream, publicID=publicID, format=format)
            self.scutter(publicID, creator)
        else:
            context.load(absolute_location, publicID=publicID, format=format)

        self.index.remove((context_id, REDFOOT.source, None))
        self.index.add((context_id, REDFOOT.source, location))
        self.index.remove((context_id, REDFOOT.publicID, None))        
        self.index.add((context_id, REDFOOT.publicID, publicID))
        if format=="xml":
            self.index.add((publicID.defrag(), RDF.type, URIRef("http://redfoot.net/rdf#RDFXMLDocument")))
        return context

    def get_context(self, identifier, creator=None):
        """ Returns a Context graph for the given identifier, which
        must be a URIRef or BNode."""
        result = Graph(store=self.store, identifier=identifier, namespace_manager=self)
        self.index.remove((identifier, RDF.type, REDFOOT.DeletedContext))
        self.index.add((identifier, RDF.type, REDFOOT.Context))
        if creator and not (identifier, DC_creator, None) in self.index:
            self.index.add((identifier, DC_creator, creator))
        if (identifier, DC_created, None) not in self.index:
            self.index.add((identifier, DC_created, Literal(date_time())))
        return result

    def remove_context(self, context):
        """removes both the context and metadata about the context."""
        if isinstance(context, URIRef) or isinstance(context, BNode):
            context = self.get_context(context)
        self.index.remove((context.identifier, None, None))
        self.index.add((context.identifier, RDF.type, REDFOOT.DeletedContext))        
        super(Redfoot, self).remove_context(context)

    def module(self, subject):
        if not isinstance(subject, URIRef):
            subject = URIRef(subject)
        module_name = subject       
        module = sys.modules.get(module_name, None)
        if module:
            return module
        self.check(subject)
        _logger.debug("importing: %s" % subject)
        value = self.value(subject)
        assert value!=None, "No objects found for %s, %s" % (subject, RDF.value)
        safe_module_name = "subject_%s" % hash(subject)
        module = ModuleType(safe_module_name)
        module.__name__ = module_name 
        module.__file__ = subject
        module.__ispkg__ = 0
        sys.modules[module_name] = module
        code = compile(value+"\n", subject, "exec")
        for key, value in self.__context.items():
            module.__dict__[key] = value
        module.__dict__["__uri__"] = subject
        exec code in module.__dict__
        return sys.modules[module_name]

    def execute(self, code, context=None, **args):
        self.check(code)
        value = self.value(code)
        assert value, "%s has no RDF.value" % code
        assert value.datatype==REDFOOT.Python, "%s RDF.value is not of datatype REDFOOT.Python. %s currently only supports REDFOOT.Python code values" % (code, redfoot_program)
        if context==None:
            context = dict(self.__context)
        for k, v in args.items():
            context[k] = v
        context["redfoot_current"] = code
        context["redfoot_program"] = redfoot_program

        value = value.replace("\r\n", "\n")        
        value = value.replace("\r", "\n")        

        c = compile(value+"\n", code, "exec")
        exec c in context

        return context

    def context_id(self, uri, context_id=None):
        """ URI#context """
        uri, frag = urldefrag(uri)
        frag = context_id or frag or "context"
        if frag.startswith("#"):
            frag = frag[1:]
        return URIRef("%s#%s" % (uri, frag))

    def namespace(self, uri, creator=None, context=None):
        uri = URIRef(uri)
        module = sys.modules.get(uri, None)
        if module is None:
            module = sys.modules[uri] = ModuleType("namespace_%s" % hash(uri))
            module.__name__ = uri 
            module.__file__ = uri
            module.__ispkg__ = 0
            if context is None:
                self.check(uri, creator=creator)
                context_uri = self.context_id(uri)            
                context = self.get_context(context_uri, creator=creator)            
            d = module.__dict__
            d["NS"] = uri
            # TODO: module.__getattribute__ = module.__dict__.__getitem__ 
            for subject in set(context.subjects(None, None)):
                if subject.startswith(uri):
                    ns, qname = subject.split(uri)
                    d[qname] = subject
        return module

    def bind(self, prefix, namespace, override=True):
        """
        Override Graphs bind to additionally declare the namespace to
        be of RDF.type REDFOOT.Namespce. And to load it if it's not
        already been loaded.
        """
        namespace = URIRef(namespace)
        if not (namespace, RDF.type, REDFOOT.Namespace) in self.index:
            try:
                self.check(namespace)
            except Exception, e:
                _logger.exception(e)
            self.index.add((namespace, RDF.type, REDFOOT.Namespace))
        super(Redfoot, self).bind(prefix, namespace, override)


    def subclasses(self, uri, direct=True):
        """
        Applies brute force RDFS entailment to infer additional classification 
        """
        if uri==RDFS.Resource:
            # TODO: direct=False
            for subclass in self.subjects(RDF.type, RDFS.Class):
                for c in self.objects(subclass, RDFS.subClassOf):
                    if c==RDFS.Resource or not (c, RDF.type, RDFS.Class) in self:
                        yield subclass
                        break
            return
        if direct==True:
            for subclass in self.subjects(RDFS.subClassOf, uri):
                yield subclass
        else:
            seen = set()
            for subclass in self.transitive_subjects(RDFS.subClassOf, uri):
                if subclass not in seen:
                    seen.add(subclass)
                    yield subclass

    def instances(self, uri):
        seen = set()
        for instance in self.subjects(RDF.type, uri):
            if instance not in seen:
                seen.add(instance)
                yield instance
        for subclass in self.subclasses(uri, direct=False):
            for instance in self.subjects(RDF.type, subclass):
                if instance not in seen:
                    seen.add(instance)
                    yield instance

    def types(self, subject):
	"""
        generator over the types of subject ordered from most specific to least
        Applies brute force RDFS entailment to infer additional classification
        """
	seen = set()
	l = []
	for type in self.objects(subject, RDF.type):
	    for t in self.transitive_objects(type, RDFS.subClassOf):
		num = len(list(self.transitive_objects(t, RDFS.subClassOf)))
		if t not in seen:
		    seen.add(t)
		    l.append((num, t))
        if isinstance(subject, URIRef):
            for type in self.objects(subject.abstract(), RDF.type):
                for t in self.transitive_objects(type, RDFS.subClassOf):
                    num = len(list(self.transitive_objects(t, RDFS.subClassOf)))
                    if t not in seen:
                        seen.add(t)
                        l.append((num, t))
	l.sort()
	l.reverse()
	for num, type in l:
	    yield type

    def possible_properties(self, type):
	for object in self.transitive_objects(type, RDFS.subClassOf):
	    for subject in self.subjects(RDFS.domain, object):
		for property in self.transitive_subjects(RDFS.subPropertyOf, subject): 
		    yield property

    def possible_properties_for_subject(self, subject):
	seen = set()
	for type in chain([RDFS.Resource], self.objects(subject, RDF.type)):
	    for property in self.possible_properties(type):
		if not property in seen:
		    seen.add(property)
		    yield property

    def main(self, options, args):
        self.check(redfoot_program) # to get default program
        if options.program:
            program = URIRef(options.program)
        else:
            program = self.program
        try:
            self.check(program)
        except ImportError, e: # TODO: something more specific that Exception
            _logger.warning("could not find program for '%s': %s" % (program, e))
        else:
            _logger.info("running: %s ( %s )" % (self.label(program), program))
            self.execute(program, args=args)

    def write(self, s, encoding="utf-8"):
        sys.stdout.write(s.encode(encoding))

    def __get_xmpp_id(self):
        if self.__xmpp_id is None:
            self.__xmpp_id = redfoot_loader.value(REDFOOT.Globals, REDFOOT.xmpp_id)
        return self.__xmpp_id
    def __set_xmpp_id(self, xmpp_id):
	redfoot_loader.remove((REDFOOT.Globals, REDFOOT.xmpp_id, None))
	redfoot_loader.add((REDFOOT.Globals, REDFOOT.xmpp_id, xmpp_id))
    xmpp_id = property(__get_xmpp_id, __set_xmpp_id)

    def __get_xmpp_password(self):
        if self.__xmpp_password is None:
            self.__xmpp_password = redfoot_loader.value(REDFOOT.Globals, REDFOOT.xmpp_password)
        return self.__xmpp_password
    def __set_xmpp_password(self, xmpp_password):
	redfoot_loader.remove((REDFOOT.Globals, REDFOOT.xmpp_password, None))
	redfoot_loader.add((REDFOOT.Globals, REDFOOT.xmpp_password, xmpp_password))
    xmpp_password = property(__get_xmpp_password, __set_xmpp_password)

    def __get_xmpp_host(self):
        if self.__xmpp_host is None:
            self.__xmpp_host = redfoot_loader.value(REDFOOT.Globals, REDFOOT.xmpp_host)
        return self.__xmpp_host
    def __set_xmpp_host(self, xmpp_host):
	redfoot_loader.remove((REDFOOT.Globals, REDFOOT.xmpp_host, None))
	redfoot_loader.add((REDFOOT.Globals, REDFOOT.xmpp_host, xmpp_host))
    xmpp_host = property(__get_xmpp_host, __set_xmpp_host)

    def __get_xmpp_port(self):
        if self.__xmpp_port is None:
            self.__xmpp_port = redfoot_loader.value(REDFOOT.Globals, REDFOOT.xmpp_port)
        return self.__xmpp_port
    def __set_xmpp_port(self, xmpp_port):
	redfoot_loader.remove((REDFOOT.Globals, REDFOOT.xmpp_port, None))
	redfoot_loader.add((REDFOOT.Globals, REDFOOT.xmpp_port, xmpp_port))
    xmpp_port = property(__get_xmpp_port, __set_xmpp_port)

    def _get_xmpp(self):
        if self.__xmpp is None:
            xmpp_uri = URIRef("modules/xmpp", base=redfoot_program)
            redfoot.check(xmpp_uri) # to pull in xmpp_info command
            if self.xmpp_id and self.xmpp_password and self.xmpp_host and self.xmpp_port:
                xmpp = self.module(URIRef("#client", base=xmpp_uri))
                self.__xmpp = xmpp.Client(self.xmpp_id, self.xmpp_password, self.xmpp_host, int(self.xmpp_port))
            else:
                _logger.info("To enable xmpp support use the redfoot xmpp_info command to set the needed information.")
        return self.__xmpp
    xmpp = property(_get_xmpp)

    def _get_rule_engine(self):
        if self.__rule_engine is None:
            rules_uri = URIRef("modules/rules", base=redfoot_program)
            rule_module = self.module(URIRef("#module", base=rules_uri))            
            rules = set(get_rules(redfoot))
            self.__rule_engine = rule_module.RuleEngine(redfoot)
        return self.__rule_engine
    rule_engine = property(_get_rule_engine)


from optparse import OptionParser
parser = OptionParser("usage: %prog program <program_options>")
parser.add_option("--path", dest="path", help="path to database") 
parser.add_option("--program", dest="program", help="URIRef of program for kernel to load and run. Defaults to command_runner")
parser.add_option("--rebuild-from-journal", action="store_true", dest="rebuild_from_journal", help="rebuild the store from the journal file")    

parser.allow_interspersed_args = False

(options, args) = parser.parse_args(args)

path = "__rfdb__"

redfoot = Redfoot("Sleepycat")

if options.rebuild_from_journal:
    _logger.info("rebuilding store from journal file...")
    os.rename(path, "%s-backup-%s" % (path, date_time()))
    redfoot.open(path)
    JournalReader(redfoot.store, "%s-journal" % path)
    redfoot.close()
    _logger.info("done rebuilding.")

journal = JournalWriter(redfoot.store, filename="%s-journal" % path) 
redfoot.open(path)
redfoot._bootstrap()
redfoot.load(redfoot_loader.program)
redfoot_loader.redfoot = redfoot 

#pyinc = PychinkoInc(redfoot.store)



try:
    _xmpp_handler = None
    if redfoot.xmpp:
        xmpp = redfoot.module(URIRef("modules/xmpp#logging", base=redfoot_program))
        _root_logger = logging.getLogger()
        _xmpp_handler = xmpp.XMPPHandler(REDFOOT.Admin)
        _xmpp_formatter = logging.Formatter('[%(name)s] %(message)s')
        _xmpp_handler.setFormatter(_xmpp_formatter)
        _root_logger.addHandler(_xmpp_handler)
except Exception, e:
    _logger.warning("XMPP logger not installed: %s" % e)

redfoot.main(options, args)

# The XMPPHandler relies on the store being open. So we must remove it before closing the store.
if _xmpp_handler:
    _root_logger.removeHandler(_xmpp_handler)

redfoot.close()

]]>
    </rdf:value>
  </redfoot:Kernel>


</rdf:RDF>  
